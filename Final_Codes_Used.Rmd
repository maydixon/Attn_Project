---
title: "Final Codes used"
author: May

output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

[libraries and data import]
```{r,  include=FALSE}

library(lme4)
library(multcomp)
library(multcompView)
library(lsmeans)
library(ggplot2)
library(dplyr)

```
### INITIAL INTEREST
[Data wrangling]
```{r, echo=FALSE,include=FALSE}
#import dataset from github

attn_int_2<- read.table("https://raw.githubusercontent.com/maydixon/Attn_Project/master/attn_for_initial_interest_2.7.17.txt", sep="\t", header=TRUE) #on github

```
[model deciding]
```{r, eval=FALSE}
#Trying to get random effects structure, and model type, mainly. 
initial_int_a <- glmer(all_tw_set ~ first_stim -1 + (1|trial_number)+(1|Bat_ID), family= "poisson", data = attn_int_2)  #no intercept

initial_int_b <- glmer(all_tw_set ~ first_stim -1 + (trial_number|Bat_ID), family= "poisson", data = attn_int_2)  #no intercept
sii5<-summary(initial_int_5)

initial_int_c <- glmer(all_tw_set ~ first_stim -1 + (0+trial_number|Bat_ID), family= "poisson", data = attn_int_2)  #not better
sii5<-summary(initial_int_5)

initial_int_nba <- glmer.nb(all_tw_set ~ first_stim -1 + (1|trial_number)+(1|Bat_ID),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  # glmmnb fits much better

initial_int_nbb <- glmer.nb(all_tw_set ~ first_stim -1 + (trial_number|Bat_ID),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  #maximal random effects structure that makes sense: results in singularity


initial_int_nbc <- glmer.nb(all_tw_set ~ first_stim -1 + (0+trial_number|Bat_ID),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  #lowest AIC, and most compex random effects structure that makes sense and converges.


initial_int_nbd <- glmer.nb(all_tw_set ~ first_stim -1 + (1|Bat_ID) + (0+trial_number|Bat_ID),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  #has singularity


 initial_int_nbd <- glmer.nb(all_tw_set ~ first_stim -1 + (1|Bat_ID) + (1|Bat_ID:trial_number),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  #has singularity

#Random effects structure **lowest AIC and BIC, small variance in random effects
initial_int_nbc <- glmer.nb(all_tw_set ~ first_stim -1 + (-1+trial_number|Bat_ID),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  #no intercept
####
cld(lsmeans(initial_int_nbc, ~"first_stim"))

#which best fitting
anova(initial_int_a, initial_int_nbb ) #nested random effects fits better
anova(initial_int_nbb, initial_int_nbc ) #negative binomial fits better
#nbb not significantly better than nbc
#But checking convergence issue below, it is overfit- has singularities


#checking convergence issues:

#singularity check
diag.vals <- getME(initial_int_nbd,"theta")[getME(initial_int_3,"lower") == 0]
any(diag.vals < 1e-6) # FALSE, no singularity

#hessian check
## 3. recompute gradient and Hessian with Richardson extrapolation
      devfun <- update(initial_int_nbd, devFunOnly=TRUE)
      if (isLMM(initial_int_3)) {
          pars <- getME(initial_int_3,"theta")
      } else {
          ## GLMM: requires both random and fixed parameters
          pars <- getME(initial_int_3, c("theta","fixef"))
      }
      if (require("numDeriv")) {
          cat("hess:\n"); print(hess <- hessian(devfun, unlist(pars)))
          cat("grad:\n"); print(grad <- grad(devfun, unlist(pars)))
          cat("scaled gradient:\n")
          print(scgrad <- solve(chol(hess), grad))
      }
## compare with internal calculations:
initial_int_3@optinfo$derivs
## pretty much the same

# 4. restart the fit from the original value (or
## a slightly perturbed value):
initial_int_restart <- update(initial_int_3, start=pars)
BM<-bootMer(initial_int_nbc, FUN=fixef, nsim=10) ## doesn't help

## 5. try all available optimizers

  source(system.file("utils", "allFit.R", package="lme4"))
  allfit.ii3 <- allFit(initial_int_nbd)
  ss <- summary(allfit.ii3)
  ss$ fixef               ## extract fixed effects
  ss$ llik                ## log-likelihoods
  ss$ sdcor               ## SDs and correlations
  ss$ theta               ## Cholesky factors
  ss$ which.OK            ## which fits worked

  ## So it seems like I can rule out model fit and say that this is false convergence positive. 
  
```

Final model:
```{r, echo=TRUE}
initial_int_nbc <- glmer.nb(all_tw_set ~ first_stim -1 + (0+trial_number|Bat_ID),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  #lowest AIC, and most compex random effects structure that makes sense and converges.
#random slope of trial number within bat ID: no variation in intercept
```

Getting estimates and p-values for differences
```{r}
int_lsmeans<- lsmeans::lsmeans(initial_int_nbc, "first_stim")
sv_int_nbc<- cld(int_lsmeans)
sv_int_nbc
pairs(int_lsmeans)

plot(int_lsmeans, comparisons=TRUE, alpha = 0.05)
```

plot of initial interest:
```{r}
#reordering for plotting: 
      attn_int_2$first_stim <- factor(attn_int_2$first_stim,levels = c("Ts" ,"T1c", "T2c" , "De", "Ra" , "Rra","Rt" ),ordered = TRUE)
      
#perhaps best order is: Ts, T, Tc, Rt, Rra, Ra, De, can do using relevel() 

#reference colors for plots
#can change all from here
Ts  = "#0E0E0E"
T1c ="#2B2B2B"
T2c = "#474747"
Rt  ="#636363"
De  = "#D5D5D5"
Ra  ="#9C9C9C"
Rra ="#808080"      

      
int.plot<- ggplot(data = attn_int_2, aes(x = first_stim, y = log(all_tw_set+1), fill=first_stim))
int.plot<- int.plot+ geom_boxplot()
int.plot<- int.plot+ geom_point(position = position_jitter(width = 0.1))
int.plot<- int.plot+ theme(axis.text.x = element_text(angle = 45, hjust=1), legend.position = "none") #format x-axis, and remove side legend #hjust 1 aligns labels
int.plot<- int.plot+ scale_x_discrete(labels=c("0c P. pustulosus","1c P. pustulosus","2c pustulosus","D. ebraccatus", "R. alata", "rev R. alata", "rev P. pustulosus"))  +
              scale_fill_manual(breaks = c("0c P. pustulosus","1c P. pustulosus","2c pustulosus","D. ebraccatus", "R. alata", "rev R. alata", "rev P. pustulosus"),
                                values=c(Ts, T1c, T2c, De, Ra, Rra, Rt))
int.plot<- int.plot+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))  #gets rid of background
int.plot<- int.plot+ xlab("Acoustic stimuli")
int.plot<- int.plot+ ylab("log( twitches in first set + 1)")
int.plot

int.plot2<- ggplot(data = attn_int_2, aes(x = first_stim, y = all_tw_set, fill=first_stim))
int.plot2<- int.plot2+ geom_boxplot()
int.plot2<- int.plot2+ geom_point(position = position_jitter(width = 0.1))
int.plot2<- int.plot2+ theme(axis.text.x = element_text(angle = 45, hjust=1), legend.position = "none") #format x-axis, and remove side legend #hjust 1 aligns labels
int.plot2<- int.plot2+ scale_x_discrete(labels=c("0c P. pustulosus","1c P. pustulosus","2c pustulosus","D. ebraccatus", "R. alata", "rev R. alata", "rev P. pustulosus"))  +
              scale_fill_manual(breaks = c("0c P. pustulosus","1c P. pustulosus","2c pustulosus","D. ebraccatus", "R. alata", "rev R. alata", "rev P. pustulosus"),
                                values=c(Ts, T1c, T2c, De, Ra, Rra, Rt))
int.plot2<- int.plot2+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))  #gets rid of background
int.plot2<- int.plot2+ xlab("Acoustic stimuli")
int.plot2<- int.plot2+ ylab(" twitches in first set")
int.plot2
```
### HABITUATION:

[Data wrangling]
```{r, echo=FALSE,include=FALSE}
#import dataset from github

bat_data<-read.table("https://raw.githubusercontent.com/maydixon/Attn_Project/master/attn_habit_1.txt", sep="\t", header=TRUE) #web

#recode and scale some variables
#need to specify dplyr bc masked by other packages
bat_data$primary_stim<- bat_data$trial_name_2
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, t_ra_t = "ts")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, de_t_de = "de", tc_ts_tc = "t2c")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, tc_ts_tc = "t2c")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, t_de_t = "ts")

bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, tia_tib_tia = "t1c")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, rt_rra_rt = "rts")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, ts_tc_ts = "ts")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, rra_rt_rra = "rra")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, ra_t_ra = "ra")

#center and scale time variable
bat_data$scaled_call_num_2<- scale(bat_data$call_num_2, center=TRUE, scale=TRUE)  #scaling call number

```

####model:
```{r, eval= FALSE}
bat.mod0 <- glmer(all_tw_call ~ scaled_call_num_2*primary_stim -1 + (trial_number| Bat_ID), data= bat_data, family="poisson")

#Troubleshooting FIT:
## https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html ##I use "gold standard" of comparing diff optimizers
  
 #1 center and scale (check) 
#2, check for singularity:
diag.vals <- getME(bat.mod0,"theta")[getME(bat.mod0,"lower") == 0]
any(diag.vals < 1e-6) # FALSE , no singularity

##3. recompute gradient and Hessian with Richardson extrapolation
devfun <- update(bat.mod0, devFunOnly=TRUE)
if (isLMM(bat.mod0)) {
    pars <- getME(bat.mod0,"theta")
} else {
    ## GLMM: requires both random and fixed parameters
    pars <- getME(bat.mod0, c("theta","fixef"))
}
if (require("numDeriv")) {
    cat("hess:\n"); print(hess <- hessian(devfun, unlist(pars)))
    cat("grad:\n"); print(grad <- grad(devfun, unlist(pars)))
    cat("scaled gradient:\n")
    print(scgrad <- solve(chol(hess), grad))
}
## compare with internal calculations:
bat.mod0@optinfo$derivs 

#So it looks to me that all the scaled gradients in the shorthand calculations are much larger than the lme4 default internal calculations, which would be one reason why model doesn't converge.

## 4. restart the fit from the original value (or
## a slightly perturbed value):
bat.mod0.restart <- update(bat.mod0, start=pars) #this also seems to fix the problem. Gets stuck on an optimum?

## 5. try all available optimizers (from Bolker)/ "gold standard"

  source(system.file("utils", "allFit.R", package="lme4"))
  fm1.all <- allFit(bat.mod0)
  ss <- summary(fm1.all)
  ss$ fixef               ## extract fixed effects #differ at most on the order od 10^-3
  ss$ llik                ## log-likelihoods #check, differ by less than .1
  ss$ sdcor               ## SDs and correlations
  ss$ theta               ## Cholesky factors
  ss$ which.OK            ## which fits worked


  ## This suggests that the model is appropriate, and the convergence warnings are a false positive, bc the different optimizers produce very similar results-- fixed effects that differ on the scale of 10^-3 or less, and identical log-likelihoods (<.1)
 
  ## Here on out I am going to use the bobyqa, though any can be used, bc they all show the name thing. (bootstrap will be vetter if I pick one that converges more, bc more will not fail. ) 
```

Final model:
```{r}
set.seed(50)
bat.mod <- glmer(all_tw_call ~ scaled_call_num_2*primary_stim -1 + (trial_number| Bat_ID), data= bat_data, family="poisson", control=glmerControl(optimizer = "bobyqa"))

sbat.mod<- summary(bat.mod)
sbat.mod


## It's a good model, Brent

######### REFS#######################
## Removing intercept
## https://stats.stackexchange.com/questions/117641/how-trustworthy-are-the-confidence-intervals-for-lmer-objects-through-effects-pa 

##ok to remove intercept
## https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model 

## Assessing fit
##https://stackoverflow.com/questions/33670628/solution-to-the-warning-message-using-glmer

 ##using bobyqa optimizer
## https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html ##I use "gold standard" of comparing diff optimizers  #using optimizer

## onlstrends and  transformation
## https://cran.r-project.org/web/packages/lsmeans/vignettes/using-lsmeans.pdf) 

###################################
```


####post hoc comparisons with lsmeans:
log scale:
```{r}
#     list estimates, se, df, 95CIS
lst=lsmeans::lstrends(bat.mod, ~primary_stim, var="scaled_call_num_2")

#      pairwise comparisons:
pairslst<- pairs(lst)

#     Show which are different from one another in letter form:
svlst<-cld(lst)
svlst
```
pairwise comparisons:
```{r}
pairslst
```

####Comparing confidence intervals by other means
Run chunk to compare
```{r}
## just making a bunch  of confidence intervals with different methods

c0 <- confint(bat.mod,method="Wald")
#ran with no errors
#c1 <- confint(bat.mod) #perc method, I belive, takes a long time to run
 #in the meantime, so i don't have to rerun it:
c1 <- read.csv("https://raw.githubusercontent.com/maydixon/Attn_Project/master/c0_boot_ouput_mac.csv", sep=",", header=TRUE)
namesc1<-(c1[,1])
c1<-c1[,-1]
rownames(c1)<-namesc1
#c1

#c2test<- confint(bat.mod, method="boot") #Takes a long time to run
      ## 1 run failed, 6 warnings
## same pre-run:
c2 <- read.csv("https://raw.githubusercontent.com/maydixon/Attn_Project/master/c2test_boot_output_mac.csv", sep=",", header=TRUE)
namesc2<-(c2[,1])
c2<-c2[,-1]
rownames(c2)<-namesc2


svlstx<-cld(lst,sort=FALSE)
c3<-as.data.frame(cbind(svlstx$asymp.LCL, svlstx$asymp.UCL))
row.names(c3)<- svlstx$primary_stim



tmpf <- function(method,val) {
    data.frame(method=method,
               v=row.names(c3),
               setNames(as.data.frame(tail(val,7)),
                        c("lwr","upr")))
}

library(ggplot2); theme_set(theme_bw())
allCI <- rbind(tmpf("lme4_wald",c0),
      tmpf("lme4_prof",c1),
      tmpf("lme4_boot",c2),
      tmpf("lstrends",c3)
      )

confint_comp<- ggplot(allCI,aes(v,ymin=lwr,ymax=upr,colour=method))+
    geom_linerange(position=position_dodge(width=0.8)) +
                     theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))

confint_comp
#ggsave("CI_comparison_final.png",width=10)
```

The different methods of generating CI's have slightly different numbers, but the gist is pretty much the same. de is higher than everything except t2c, rra and ra are lower than everything, and the others are the same and in the middle. Boot maybe has de and t2c being different, Wald has De not different than various forward tungaras.  

I'm using lsmean's values bc they generate tukey adjusted  p-vals and sig codes as well


### Plot of just lstrends output

```{r}

svlstp<-svlst #ordering factor to change position of plots
svlstp$primary_stim<-factor(svlstp$primary_stim, levels = c("ts" ,"t1c", "t2c" , "de", "ra" , "rra","rts"), ordered=TRUE)

confint_comp_ls<- ggplot(svlstp, aes(primary_stim,ymin=asymp.LCL,ymax=asymp.UCL,colour=primary_stim))+
    geom_linerange() +
 theme(panel.grid.major = element_blank(), panel.grid.minor =  element_blank(),panel.background = element_blank(),  axis.line = element_line(colour = "black"),
legend.position = "none", axis.text.x = element_text(angle = 45, hjust=1))  + #gets rid of background
scale_x_discrete(labels=c("0c P. pustulosus","1c P. pustulosus","2c pustulosus","D. ebraccatus", "R. alata", "rev R. alata", "rev P. pustulosus"))  +
              scale_colour_manual(breaks = c("0c P. pustulosus","1c P. pustulosus","2c pustulosus","D. ebraccatus", "R. alata", "rev R. alata", "rev P. pustulosus"),
                                values=c(Ts, T1c, T2c, De, Ra, Rra, Rt)) +
#scale_fill_manual(breaks = c("ts","t1c","t2c","de", "ra", "rra", "rts"),
                             #   values=c(ts, t1c, t2c, de, ra, rra, rts)) +

 xlab("Call Type") +
ylab("Slope of change in response to calls in log scale")

confint_comp_ls
ggsave("habituation_slope_plot_grayscale.png",width=10)
```
