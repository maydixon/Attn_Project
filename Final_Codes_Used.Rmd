---
title: "Final Codes used"
author: May

output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

[libraries and data import]
```{r,  include=FALSE}

library(lme4)
library(multcomp)
library(multcompView)
library(lsmeans)
library(ggplot2)
library(dplyr)

```

###palette for plotting all, assigning variables
```{r}
#Checking out a palette of colors:
        plotCol(gray(1.1:7.8 /9))
#finding the name of those colors:
      gray(1.1:7.8 /9)
      
#reference colors for plots
#can change all from here
Ts  = "#1F1F1F"
T1c ="#3C3C3C"
T2c = "#585858"
De  = "#747474"
Ra  ="#919191"
Rra ="#ADADAD"  
Rt  ="#C9C9C9"

    


```

### INITIAL INTEREST
[Data wrangling]
```{r, echo=FALSE,include=FALSE}
#import dataset from github

attn_int_2<- read.table("https://raw.githubusercontent.com/maydixon/Attn_Project/master/attn_for_initial_interest_2.7.17.txt", sep="\t", header=TRUE) #on github
attn_int_2<- subset(attn_int_2, attn_int_2$bat_name != "Blackbeard" ) #removes unresponsive bat
#fix levels of trial number

```
[model deciding]
```{r, eval=FALSE}
#Trying to get random effects structure, and model type, mainly. 
initial_int_a <- glmer(all_tw_set ~ first_stim -1 + (1|trial_number)+(1|Bat_ID), family= "poisson", data = attn_int_2)  #no intercept

initial_int_b <- glmer(all_tw_set ~ first_stim -1 + (trial_number|Bat_ID), family= "poisson", data = attn_int_2)  #no intercept
sii5<-summary(initial_int_5)

initial_int_c <- glmer(all_tw_set ~ first_stim -1 + (0+trial_number|Bat_ID), family= "poisson", data = attn_int_2)  #not better
sii5<-summary(initial_int_5)

initial_int_nba <- glmer.nb(all_tw_set ~ first_stim -1 + (1|trial_number)+(1|Bat_ID),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  # glmmnb fits much better

initial_int_nbb <- glmer.nb(all_tw_set ~ first_stim -1 + (trial_number|Bat_ID),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  #maximal random effects structure that makes sense: results in singularity


initial_int_nbc <- glmer.nb(all_tw_set ~ first_stim -1 + (0+trial_number|Bat_ID),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  #lowest AIC, and most compex random effects structure that makes sense and converges.


initial_int_nbd <- glmer.nb(all_tw_set ~ first_stim -1 + (1|Bat_ID) + (0+trial_number|Bat_ID),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  #has singularity


 initial_int_nbd <- glmer.nb(all_tw_set ~ first_stim -1 + (1|Bat_ID) + (1|Bat_ID:trial_number),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  #has singularity

#Random effects structure **lowest AIC and BIC, small variance in random effects
initial_int_nbc <- glmer.nb(all_tw_set ~ first_stim -1 + (-1+trial_number|Bat_ID),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  #no intercept
####
cld(lsmeans(initial_int_nbc, ~"first_stim"))

#which best fitting
anova(initial_int_a, initial_int_nbb ) #nested random effects fits better
anova(initial_int_nbb, initial_int_nbc ) #negative binomial fits better
#nbb not significantly better than nbc
#But checking convergence issue below, it is overfit- has singularities


#checking convergence issues:

#singularity check
diag.vals <- getME(initial_int_nbd,"theta")[getME(initial_int_3,"lower") == 0]
any(diag.vals < 1e-6) # FALSE, no singularity

#hessian check
## 3. recompute gradient and Hessian with Richardson extrapolation
      devfun <- update(initial_int_nbd, devFunOnly=TRUE)
      if (isLMM(initial_int_3)) {
          pars <- getME(initial_int_3,"theta")
      } else {
          ## GLMM: requires both random and fixed parameters
          pars <- getME(initial_int_3, c("theta","fixef"))
      }
      if (require("numDeriv")) {
          cat("hess:\n"); print(hess <- hessian(devfun, unlist(pars)))
          cat("grad:\n"); print(grad <- grad(devfun, unlist(pars)))
          cat("scaled gradient:\n")
          print(scgrad <- solve(chol(hess), grad))
      }
## compare with internal calculations:
initial_int_3@optinfo$derivs
## pretty much the same

# 4. restart the fit from the original value (or
## a slightly perturbed value):
initial_int_restart <- update(initial_int_3, start=pars)
BM<-bootMer(initial_int_nbc, FUN=fixef, nsim=10) ## doesn't help

## 5. try all available optimizers

  source(system.file("utils", "allFit.R", package="lme4"))
  allfit.ii3 <- allFit(initial_int_nbd)
  ss <- summary(allfit.ii3)
  ss$ fixef               ## extract fixed effects
  ss$ llik                ## log-likelihoods
  ss$ sdcor               ## SDs and correlations
  ss$ theta               ## Cholesky factors
  ss$ which.OK            ## which fits worked

  ## So it seems like I can rule out model fit and say that this is false convergence positive. 
  
```

Final model:
```{r, echo=TRUE}
initial_int_nbc <- glmer.nb(all_tw_set ~ first_stim -1 + (0+trial_number|Bat_ID),  data = attn_int_2, control=glmerControl(optimizer = "bobyqa"))  #lowest AIC, and most compex random effects structure that makes sense and converges.
#random slope of trial number within bat ID: no variation in intercept
```

Getting estimates and p-values for differences
```{r}
int_lsmeans<- lsmeans::lsmeans(initial_int_nbc, "first_stim")
sv_int_nbc<- cld(int_lsmeans, letters=LETTERS)
sv_int_nbc
```
pairwise comparisons:
```{r}
pairs(int_lsmeans)
```
Plot of confidence intervals, overlapping arrows are not significantly different:
```{r}
plot(int_lsmeans, comparisons=TRUE, alpha = 0.05)
```


plot of initial interest:
```{r}
#reordering for plotting: 
      attn_int_2$first_stim <- factor(attn_int_2$first_stim,levels = c("Ts" ,"T1c", "T2c" , "De", "Ra" , "Rra","Rt" ),ordered = TRUE)
      
#perhaps best order is: Ts, T, Tc, Rt, Rra, Ra, De, can do using relevel() 


int.plot<- ggplot(data = attn_int_2, aes(x = first_stim, y = all_tw_set, fill=first_stim))
int.plot<- int.plot+ geom_boxplot()
int.plot<- int.plot+ geom_point(position = position_jitter(width = 0.1))
int.plot<- int.plot+ theme(axis.text.x = element_text(angle = 45, hjust=1), legend.position = "none") #format x-axis, and remove side legend #hjust 1 aligns labels
int.plot<- int.plot+ scale_x_discrete(labels=c("0c P. pustulosus","1c P. pustulosus","2c pustulosus","D. ebraccatus", "R. alata", "rev R. alata", "rev P. pustulosus"))  +
              scale_fill_manual(breaks = c("0c P. pustulosus","1c P. pustulosus","2c pustulosus","D. ebraccatus", "R. alata", "rev R. alata", "rev P. pustulosus"),
                                values=c(Ts, T1c, T2c, De, Ra, Rra, Rt))
int.plot<- int.plot+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))  #gets rid of background
int.plot<- int.plot+ xlab("Acoustic stimuli")
int.plot<- int.plot+ ylab("Ear-twitches in trial 1")
int.plot

```

### HABITUATION:

[Data wrangling]
```{r, echo=FALSE,include=FALSE}
#import dataset from github
bat_data<-read.table("https://raw.githubusercontent.com/maydixon/Attn_Project/master/attn_habit_4.txt", sep="\t", header=TRUE) #web
#* local:
bat_data<-read.table("/Users/maydixon/GitHub/Attn_Project/Attn_project_github/attn_habit_4.txt", sep="\t", header=TRUE) #local

bat_data<- subset(bat_data, bat_data$bat_name != "Blackbeard" ) #removes unresponsive bat


#* bat_data<-read.table("https://raw.githubusercontent.com/maydixon/Attn_Project/master/attn_habit_1.txt", sep="\t", header=TRUE) #web

#recode and scale some variables
#need to specify dplyr bc masked by other packages
bat_data$primary_stim<- bat_data$trial_name_2
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, t_ra_t = "ts")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, de_t_de = "de", tc_ts_tc = "t2c")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, tc_ts_tc = "t2c")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, t_de_t = "ts")

bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, tia_tib_tia = "t1c")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, rt_rra_rt = "rts")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, ts_tc_ts = "ts")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, rra_rt_rra = "rra")
bat_data$primary_stim<-dplyr::recode(bat_data$primary_stim, ra_t_ra = "ra")

#center and scale time variable
bat_data$scaled_call_num_2<- scale(bat_data$call_num_2, center=TRUE, scale=TRUE)  #scaling call number

#*outs <- boxplot(all_tw_set ~ Bat_ID,  data=bat_data)
#*boxplot(bat_data$bat_ID, bat_data$all_tw_trial )
```

####model:
```{r, eval= FALSE}
bat.mod0 <- glmer(all_tw_call ~ scaled_call_num_2*primary_stim -1 + (trial_number| Bat_ID), data= bat_data, family="poisson")
#Troubleshooting FIT:
## https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html ##I use "gold standard" of comparing diff optimizers
  
 #1 center and scale (check) 
#2, check for singularity:
diag.vals <- getME(bat.mod0,"theta")[getME(bat.mod0,"lower") == 0]
any(diag.vals < 1e-6) # FALSE , no singularity

##3. recompute gradient and Hessian with Richardson extrapolation
devfun <- update(bat.mod0, devFunOnly=TRUE)
if (isLMM(bat.mod0)) {
    pars <- getME(bat.mod0,"theta")
} else {
    ## GLMM: requires both random and fixed parameters
    pars <- getME(bat.mod0, c("theta","fixef"))
}
if (require("numDeriv")) {
    cat("hess:\n"); print(hess <- hessian(devfun, unlist(pars)))
    cat("grad:\n"); print(grad <- grad(devfun, unlist(pars)))
    cat("scaled gradient:\n")
    print(scgrad <- solve(chol(hess), grad))
}
## compare with internal calculations:
bat.mod0@optinfo$derivs 

#So it looks to me that all the scaled gradients in the shorthand calculations are much larger than the lme4 default internal calculations, which would be one reason why model doesn't converge.

## 4. restart the fit from the original value (or
## a slightly perturbed value):
bat.mod0.restart <- update(bat.mod0, start=pars) #this also seems to fix the problem. Gets stuck on an optimum?

## 5. try all available optimizers (from Bolker)/ "gold standard"

  source(system.file("utils", "allFit.R", package="lme4"))
  fm1.all <- allFit(bat.mod0)
  ss <- summary(fm1.all)
  ss$ fixef               ## extract fixed effects #differ at most on the order od 10^-3
  ss$ llik                ## log-likelihoods #check, differ by less than .1
  ss$ sdcor               ## SDs and correlations
  ss$ theta               ## Cholesky factors
  ss$ which.OK            ## which fits worked


  ## This suggests that the model is appropriate, and the convergence warnings are a false positive, bc the different optimizers produce very similar results-- fixed effects that differ on the scale of 10^-3 or less, and identical log-likelihoods (<.1)
 
  ## Here on out I am going to use the bobyqa, though any can be used, bc they all show the name thing. (bootstrap will be vetter if I pick one that converges more, bc more will not fail. ) 
```

Final model:
```{r}
set.seed(50)
bat.mod <- glmer(all_tw_call ~ scaled_call_num_2*primary_stim -1 + (trial_number| Bat_ID), data= bat_data, family="poisson", control=glmerControl(optimizer = "bobyqa"))

sbat.mod<- summary(bat.mod)
sbat.mod


## It's a good model, Brent

######### REFS#######################
## Removing intercept
## https://stats.stackexchange.com/questions/117641/how-trustworthy-are-the-confidence-intervals-for-lmer-objects-through-effects-pa 

##ok to remove intercept
## https://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model 

## Assessing fit
##https://stackoverflow.com/questions/33670628/solution-to-the-warning-message-using-glmer

 ##using bobyqa optimizer
## https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html ##I use "gold standard" of comparing diff optimizers  #using optimizer

## onlstrends and  transformation
## https://cran.r-project.org/web/packages/lsmeans/vignettes/using-lsmeans.pdf) 

###################################
```


####post hoc comparisons with lsmeans:
log scale:
```{r}
#     list estimates, se, df, 95CIS
lst=lsmeans::lstrends(bat.mod, ~primary_stim, var="scaled_call_num_2")

#      pairwise comparisons:
pairslst<- pairs(lst)

#     Show which are different from one another in letter form:
svlst<-cld(lst)
svlst
```
pairwise comparisons:
```{r}
pairslst
```

####Comparing confidence intervals by other means
Run chunk to compare
```{r}
## just making a bunch  of confidence intervals with different methods

c0 <- confint(bat.mod,method="Wald")
#ran with no errors
#c1 <- confint(bat.mod) #perc method, I belive, takes a long time to run
 #in the meantime, so i don't have to rerun it:
c1 <- read.csv("https://raw.githubusercontent.com/maydixon/Attn_Project/master/c0_boot_ouput_mac.csv", sep=",", header=TRUE)
namesc1<-(c1[,1])
c1<-c1[,-1]
rownames(c1)<-namesc1
#c1

#c2test<- confint(bat.mod, method="boot") #Takes a long time to run
      ## 1 run failed, 6 warnings
## same pre-run:
c2 <- read.csv("https://raw.githubusercontent.com/maydixon/Attn_Project/master/c2test_boot_output_mac.csv", sep=",", header=TRUE)
namesc2<-(c2[,1])
c2<-c2[,-1]
rownames(c2)<-namesc2


svlstx<-cld(lst,sort=FALSE)
c3<-as.data.frame(cbind(svlstx$asymp.LCL, svlstx$asymp.UCL))
row.names(c3)<- svlstx$primary_stim



tmpf <- function(method,val) {
    data.frame(method=method,
               v=row.names(c3),
               setNames(as.data.frame(tail(val,7)),
                        c("lwr","upr")))
}

library(ggplot2); theme_set(theme_bw())
allCI <- rbind(tmpf("lme4_wald",c0),
      tmpf("lme4_prof",c1),
      tmpf("lme4_boot",c2),
      tmpf("lstrends",c3)
      )

confint_comp<- ggplot(allCI,aes(v,ymin=lwr,ymax=upr,colour=method))+
    geom_linerange(position=position_dodge(width=0.8)) +
                     theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))

confint_comp
#ggsave("CI_comparison_final.png",width=10)
```

The different methods of generating CI's have slightly different numbers, but the gist is pretty much the same. de is higher than everything except t2c, rra and ra are lower than everything, and the others are the same and in the middle. Boot maybe has de and t2c being different, Wald has De not different than various forward tungaras.  

I'm using lsmean's values bc they generate tukey adjusted  p-vals and sig codes as well


### Plot of just lstrends output

```{r}

svlstp<-svlst #ordering factor to change position of plots
svlstp$primary_stim<-factor(svlstp$primary_stim, levels = c("ts" ,"t1c", "t2c" , "de", "ra" , "rra","rts"), ordered=TRUE)

  theme_set(theme_classic())    

confint_comp_ls<- ggplot(svlstp, aes(primary_stim,ymin=asymp.LCL,ymax=asymp.UCL,colour=primary_stim))+
    geom_linerange() +
 theme(panel.grid.major = element_blank(), panel.grid.minor =  element_blank(),panel.background = element_blank(),  axis.line = element_line(colour = "black"),
legend.position = "none", axis.text.x = element_text(angle = 45, hjust=1))  + #gets rid of background
scale_x_discrete(labels=c("0c P. pustulosus","1c P. pustulosus","2c pustulosus","D. ebraccatus", "R. alata", "rev R. alata", "rev P. pustulosus"))  +
              scale_colour_manual(breaks = c("0c P. pustulosus","1c P. pustulosus","2c pustulosus","D. ebraccatus", "R. alata", "rev R. alata", "rev P. pustulosus"),
                                values=c(Ts, T1c, T2c, De, Ra, Rra, Rt)) +
#scale_fill_manual(breaks = c("ts","t1c","t2c","de", "ra", "rra", "rts"),
                             #   values=c(ts, t1c, t2c, de, ra, rra, rts)) +

 xlab("Call Type") +
ylab("log slope of change in ear-twitch response") +
      geom_hline(yintercept=0, linetype="dotted") 
      

confint_comp_ls
#ggsave("habituation_slope_plot_grayscale.png",width=3, height =4)
```


### Discrimination

[data wrangling]

```{r, include = FALSE}
# dis data wrangling 

#import data, remove duds, remove bad treatments
 attention_Rcopy_individuals_condensed <- read.delim("https://raw.githubusercontent.com/maydixon/Attn_Project/master/attention_Rcopy_individuals_condensed.txt", header=TRUE) #this file has all sets, and a row with condensed Tia_tib_Tia

attention_Rcopy1<- subset(attention_Rcopy_individuals_condensed, attention_Rcopy_individuals_condensed$bat_name != "Blackbeard" ) #removes unresponsive bat
attention_Rcopy1<-subset(attention_Rcopy1, attention_Rcopy1$trial_name != "t_rt_t" & attention_Rcopy1$trial_name != "rt_t_rt") #removes variables only tested on one bat
 attn_r2 <- attention_Rcopy1 #short same version
#pull partial string match  x.1 for call number using grep
        
attn_set <- attn_r2[grep(".1", attn_r2$call_num), ]
## This is coded terribly, but it works, ok
subset_de <- subset(attn_set, attn_set$trial_name_2=="de_t_de")
subset_ra <- subset(attn_set, attn_set$trial_name_2=="ra_t_ra")
subset_rra <- subset(attn_set, attn_set$trial_name_2=="rra_rt_rra")
subset_rt <- subset(attn_set, attn_set$trial_name_2=="rt_rra_rt")
subset_t_de <- subset(attn_set, attn_set$trial_name_2=="t_de_t")
subset_t_ra <- subset(attn_set, attn_set$trial_name_2=="t_ra_t")
subset_tc <- subset(attn_set, attn_set$trial_name_2=="tc_ts_tc")
subset_tia <- subset(attn_set, attn_set$trial_name_2=="tia_tib_tia")
subset_ts <- subset(attn_set, attn_set$trial_name_2=="ts_tc_ts")

#further subset to only include sets 5, 6, 7
subset_de1 <- subset(subset_de, subset_de$set_num=="5" | subset_de$set_num=="6" | subset_de$set_num=="7" )
subset_ra1 <- subset(subset_ra, subset_ra$set_num=="5" | subset_ra$set_num=="6" | subset_ra$set_num=="7" ) 
subset_rra1 <- subset(subset_rra, subset_rra$set_num=="5" | subset_rra$set_num=="6" | subset_rra$set_num=="7" )
subset_rt1 <- subset(subset_rt, subset_rt$set_num=="5" | subset_rt$set_num=="6" | subset_rt$set_num=="7" )
subset_t_de1 <- subset(subset_t_de, subset_t_de$set_num=="5" | subset_t_de$set_num=="6" | subset_t_de$set_num=="7" )
subset_t_ra1 <- subset(subset_t_ra, subset_t_ra$set_num=="5" | subset_t_ra$set_num=="6" | subset_t_ra$set_num=="7" )
subset_tc1 <- subset(subset_tc, subset_tc$set_num=="5" | subset_tc$set_num=="6" | subset_tc$set_num=="7" )
subset_tia1 <- subset(subset_tia, subset_tia$set_num=="5" | subset_tia$set_num=="6" | subset_tia$set_num=="7" )
subset_ts1 <- subset(subset_ts, subset_ts$set_num=="5" | subset_ts$set_num=="6" | subset_ts$set_num=="7" )

```

## Summary chart


| trial_name | descrim? |dishabit? | comments
|:----------|------|------|-------------------------------------------|
|de_t_de  | no | no | probably bc no habituation in first place x|
|t_de_t | no |no | probably bc high overall responses, low habit x|
|ra_t_ra |yes, strong | yes | model failed to converge tho x|
|t_ra_t | no | no | model didn't converge, failed hessian?x |
|rra_rt_rra | yes, strong | no | x |
|rt_rra_rt | yes | no | x |
|tc_ts_tc |no |no |  crazy outlier to deal with, possbily no habituation|
|ts_tc_ts | yes | no | more clear bc better habituation x|
| tia_tib_tia | no | no | not even at all. should have habituated too |


### *D. ebracattus* v Tungara: 
normality of data:
```{r, echo= FALSE}
qqnorm(subset_de1$all_tw_set)

```


Results of linear mixed model: This models asks whether there is a significant difference between set 5 & 6, and 5 & 7. 

```{r, echo=FALSE}
#de_desc<- lme(all_tw_set ~ as.factor(set_num), random = ~1|Bat_ID, data = subset_de1) # use as.factor so model sees these as factors
de_desc_glm <- glmer(all_tw_set ~ as.factor(set_num) + (1|Bat_ID) , data = subset_de1, family= "poisson")

```
```{r}
#summary(de_desc)
summary(de_desc_glm)
de_t_de_desc <- boxplot(all_tw_set ~ (as.factor(set_num)), main = "de_t_de", data=subset_de1)
de_lst<-lsmeans(de_desc_glm, ~set_num)
cld_de<-cld(de_lst)
cld_de
pairs(de_lst)
```


So, there is no evidence of discrimination between ebracattus and tungara. This may be because there was no habituation to D. ebracattus (actually there was slight sensitization)       


### *Rhinella alata* v Tungara
normality of data: 
```{r}
qnorm_ra<- qqnorm(subset_ra1$all_tw_set)
```
```{r, include=FALSE}
#ra_desc<- lme(all_tw_set ~ as.factor(set_num), random = ~1|Bat_ID, data = subset_ra1) # use as.factor so moral sees these as factors
ra_desc_glm <- glmer(all_tw_set ~ as.factor(set_num) + (1|Bat_ID) , data = subset_ra1, family= "poisson", control=glmerControl(optimizer = "bobyqa"))
ra_glm.nb<-glmer.nb(all_tw_set ~ as.factor(set_num) + (1|Bat_ID) , data = subset_ra1, control=glmerControl(optimizer = "bobyqa"))
anova(ra_glm.nb, ra_desc_glm)

```

```{r}
sumra<- summary(ra_desc_glm)
#boxplot
ra_descrim_plot<- boxplot(all_tw_set ~ (as.factor(set_num)), main = "ra_t_ra", data=subset_ra1)

ra_lst<-lsmeans(ra_desc_glm, ~set_num)
cld_ra<-cld(ra_lst)
cld_ra
pairs(ra_lst)
```

There is evidence for descrimination between rhinella alata and tungara frogs

### Reversed *R. alata* v tungara

Normality of data:
```{r}
qqnorm(subset_rra1$all_tw_set)
#lots of zeros, floor effects

```
```{r, include=FALSE}
rra_desc_glm <- glmer(all_tw_set ~ as.factor(set_num) + (1|Bat_ID) , data = subset_rra1, family= "poisson")
rra_desc_glm.nb <- glmer.nb(all_tw_set ~ as.factor(set_num) + (1|Bat_ID) , data = subset_rra1)
anova(rra_desc_glm, rra_desc_glm.nb)

```
```{r}
#summary(rra_desc)
summary(rra_desc_glm)
#boxplot
boxplot(all_tw_set ~ (as.factor(set_num)), main = "rra_rt_rra", data=subset_rra1)

rra_lst<-lsmeans(rra_desc_glm, ~set_num)
cld_rra<-cld(rra_lst)
cld_rra
pairs(rra_lst)
```

There is evidence for descrimination between 5 and 6, but not for dishabituation (5 and 7). There was very steep habituation to reversed rhinella alata, which might explain a part of this trend. 

So, clearly they discriminate / can discriminate to the sounds of the reversed rhinella and the reversed tungara. 


### reversed tungara v reversed *R. alata*

Normality of data: 
```{r, echo=FALSE}
qqnorm((subset_rt1$all_tw_set))
```

This linear mixed model may not be appropriate for this test given its terribly right-skewed distribution

```{r, echo=FALSE}

rt_desc_glm <- glmer(all_tw_set ~ as.factor(set_num) + (1|Bat_ID) , data = subset_rt1, family= "poisson")
#its ugly, but it converges
rt_desc_glm.nb<-glmer.nb(all_tw_set ~ as.factor(set_num) + (1|Bat_ID) , data = subset_rt1, family= "poisson")


anova(rt_desc_glm, rt_desc_glm.nb)
#in this case, the negative binomial test fits a lot betta.(dealing with the extra zeros) 
#but this is too much for the data.

diag.vals <- getME(rt_desc_glm.nb,"theta")[getME(rt_desc_glm.nb,"lower") == 0]
any(diag.vals < 1e-6) # TRUE, so singularities

```
``````{r}
summary(rt_desc_glm)
#boxplot
boxplot(all_tw_set ~ (as.factor(set_num)), main = "rt_rra_rt", data=subset_rt1)
#logged boxplot:

rt_desc_transformed<-boxplot((log(all_tw_set+1)) ~ (as.factor(set_num)), main = "rt_rra_rt log transformed", data=subset_rt1)

```

```{r}
rt_lst<-lsmeans(rt_desc_glm, ~set_num) 
cld_rt<-cld(rt_lst)
cld_rt
pairs(rt_lst)
```

So again, there is discrimination, if not dishabituation between reversed tungara and reversed alata. 


### Tungara v *D. ebracattus*

```{r, echo=FALSE}
qq.t.de<- qqnorm(subset_t_de1$all_tw_set)


t_de_desc_glm <- glmer(all_tw_set ~ as.factor(set_num) + (1|Bat_ID) , data = subset_t_de1, family= "poisson")
sum_t_de<- summary(t_de_desc_glm)
#boxplot
t_de_t_desc_plot<- boxplot(all_tw_set ~ (as.factor(set_num)), main = "t_de_t", data=subset_t_de1)
```
```{r}
t_de_lst<-lsmeans(t_de_desc_glm, ~set_num)
cld_t_de<-cld(t_de_lst)
cld_t_de
pairs(t_de_lst)
```

There is no evidence for descrimination between tungara and dendrobates Perhaps/ likley due to high overall responses

###Tungara v R. alata
```{r, echo=FALSE}

#remember to reset relevel if running this again
qqnorm(subset_t_ra1$all_tw_set) #almost half of values are zeros

t_ra_t_desc_plot<- boxplot(all_tw_set ~ (as.factor(set_num)), main = "t_ra_t", data=subset_t_ra1)



t_ra_desc_glm <- glmer(all_tw_set ~ set_num + (1|Bat_ID) , data = subset_t_ra1, family= "poisson", control=glmerControl(optimizer = "bobyqa"))
#not converging?-> converges with bobyqa

summary(t_ra_desc_glm)

```


This chunk isn't knittingn for some reason: eval=false for now
```{r, eval=FALSE}

t_ra_lst <- lsmeans(t_ra_desc_glm, ~set_num)
cld_t_ra <- cld(t_ra_lst)
cld_t_ra
pairs(t_ra_lst)

```

No discimination between treatments detected. (generalization or dishabituation)



## Intraspecific Trials

## Complex tungara v simple tungara
```{r}

qqnorm(subset_tc1$all_tw_set)
#one total outlier 
#more than half zeros!
tc_desc_glm <- glmer(all_tw_set ~ as.factor(set_num) + (1|Bat_ID) , data = subset_tc1, family= "poisson")
summary(tc_desc_glm)
# boxplot
tc_ts_tc_plot<- boxplot(all_tw_set ~ (as.factor(set_num)), main = "tc_ts_tc", data=subset_tc1)

#dealing with outlier
#View(subset_tc1)
subset_tc1$all_tw_set.no<-subset_tc1$all_tw_set
subset_tc1$all_tw_set.no <- ifelse(subset_tc1$all_tw_set.no > 15, NA, subset_tc1$all_tw_set.no) #removing points over 15
tc_ts_tc_plot.no<- boxplot(all_tw_set.no ~ (as.factor(set_num)), main = "tc_ts_tc", data=subset_tc1)
tc_desc_glm.no <- glmer(all_tw_set.no ~ as.factor(set_num) + (1|Bat_ID) , data = subset_tc1, family= "poisson")
summary(tc_desc_glm.no)
#makes nothing significant at all. Diff between last 2? 
```
with outlier
```{r}

tc_lst<-lsmeans(tc_desc_glm, ~set_num)
cld_tc<-cld(tc_lst)
cld_tc
pairs(tc_lst)
```

without outlier
```{r}

tc_lst.no<-lsmeans(tc_desc_glm.no, ~set_num)
cld_tc.no<-cld(tc_lst.no)
cld_tc.no
pairs(tc_lst.no)
```
So, with outlier 6 is significantly lower than 7, without outlier, they are all the same, regardless, no evidence of dishabituation or anything 
There is no apparent discrimination between complex and simple calls in this test. However, there is an outlier that could easily be messing with this. Outlier doesn't change the relationship. It just makes them all the more the same

If this is accurate, it is probable that discrimination is obscured by simple calls being less salient stimuli, so the interuption in habituation is masked by lower overall interest in the simple call. See the inverse treatment:

##Simple tungara against complex tungara call
```{r}

qqnorm(subset_ts1$all_tw_set)
hist(subset_ts1$all_tw_set)
#pretty 0 weighted)
ts_desc_glm <- glmer(all_tw_set ~ as.factor(set_num) + (1|Bat_ID) , data = subset_ts1, family= "poisson")

summary(ts_desc_glm)
# boxplot
ts_tc_ts_plot<- boxplot(all_tw_set ~ (as.factor(set_num)), main = "ts_tc_ts", data=subset_ts1)    

```

```{r}

ts_lst<-lsmeans(ts_desc_glm, ~set_num)
cld_ts<-cld(ts_lst)
cld_ts
pairs(ts_lst)
```


This shows that bats discriminate between complex and simple tungara calls (0c v 2c). There is clear evidence of discrimination (lack of stimulus genrealization), but not of dishabituation  Note how discrimination is more apparent when the less salient stimulus is the one that is being habituated. ((Also, more habituation=more evidence of discrimination))

## Tungara "A" v Tungara "B" (or the id of individuals trials)

```{r}

qqnorm(subset_tia1$all_tw_set)
#almost half zeros
# not justifiably normal (left/0 skewed)

tia_desc_glm <- glmer(all_tw_set ~ as.factor(set_num) + (1|Bat_ID) , data = subset_tia1, family= "poisson")
summary(tia_desc_glm)
# boxplot
tia_tib_tia_plot<- boxplot(all_tw_set ~ (as.factor(set_num)), main = "tia_tib_tia", data=subset_tia1)
```
significant differences and pairwise contrasts:
```{r}
tia_lst<-lsmeans(tia_desc_glm, ~set_num)
cld_tia<-cld(tia_lst)
cld_tia
pairs(tia_lst)

```
There is no apparent discrimination between individual tungaras. Removing the possible outliers would only make it less likely to show dishabituation 



### Attention set 5- "space to move?"
[data wrangling]
```{r}
 attn_set_5 <- read.table("/Users/maydixon/Dropbox/Attention Project/R/attn_set5_only.txt", header=TRUE) #local      

```



##Plotting set 5

```{r}
 attn_set_5$primary_stim <- factor(attn_set_5$primary_stim,levels = c("Ts" ,"T1c", "T2c" , "De", "Ra" , "Rra","Rt" ),ordered = TRUE)

p5<- ggplot(data = attn_set_5, aes(x = primary_stim, y = all_tw_set, fill=primary_stim))
p5<- p5+ geom_boxplot()
p5<- p5+ geom_point(position = position_jitter(width = 0.1))
p5<- p5+ theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = "none") #format x-axis, and remove side legend

p5<- p5+ scale_x_discrete(labels=c("0c P. pustulosus","1c P. pustulosus","2c pustulosus","D. ebraccatus", "R. alata", "rev R. alata", "rev P. pustulosus"))  +
              scale_fill_manual(breaks = c("0c P. pustulosus","1c P. pustulosus","2c pustulosus","D. ebraccatus", "R. alata", "rev R. alata", "rev P. pustulosus"),
                                values=c(Ts, T1c, T2c, De, Ra, Rra, Rt))
p5<- p5+ theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"))  #gets rid of background


p5<- p5+ xlab("Acoustic stimuli")
p5<- p5+ ylab("Ear-twitches in trial 5")
p5
ggsave("Ear twitches in set 5.jpg", width = 3, height = 4)
#ggsave("CI_comparison_final.png",width=5)
```



###Plots of whole trials

[Data wrangling]
```{r}
        attn_whole<-read.table("https://raw.githubusercontent.com/maydixon/Attn_Project/master/attention_Rcopy_individuals_condensed.txt", sep="\t", header=TRUE)
      
        attn_whole <- subset(attn_whole, attn_whole$bat_name != "Blackbeard") 
        # want just one line for each "set"
        #pull partial string match  x.1 for call number using grep
        attn_whole_set <- attn_whole[grep(".1", attn_whole$call_num), ]
```        
        
        

[de_t_de]
```{r}
      
        #making a plot for the entire of one treatment, with each set getting a boxplot
        
        #DE
        de_data<- attn_whole_set[attn_whole_set$trial_name_2=="de_t_de", ]       
        p_de <-ggplot(data=de_data, aes(x = set_num, y = all_tw_set))      
        p_de <- p_de + geom_boxplot(aes(fill=factor(set_num)))
        #p_de<- p_de + geom_point(position = position_jitter(width = 0.1))
        p_de <- p_de + guides(fill=FALSE)
        p_de <- p_de + scale_x_continuous(breaks = c(1,2,3,4,5,6,7)) +
              scale_fill_manual(breaks = c("1", "2", "3", "4", "5", "6", "7"),
                                values=c(De, De, De, De, De, Ts, De))
       # p_de <- p_de + ylab(element_blank())
       #p_de <- p_de + xlab(element_blank())
        p_de<- p_de + labs(title = "D. ebraccatus / 0-chuck P. pustulosus") +
                  theme(plot.title = element_text(face="italic", size=22)) #this makes the plot title italic, affects the size
         p_de <- p_de + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"), axis.text.y=element_blank(), axis.title.x=element_blank(),
          axis.title.y=element_blank()) #this gets rid of grey background and lines, adds 2 axes
        p_de <- p_de + coord_cartesian(ylim = c(0, 25))
        #p_de <- p_de + stat_smooth(colour="black", size=0.5, method="lm", se=FALSE ,  data= subset(de_data, de_data$set_num!="6" & de_data$set_num!="7")) 
       
        p_de
 ggsave("de_trial.jpg", height =5, width = 7)             
```


t_de
```{r}
      
        #making a plot for the entire of one treatment, with each set getting a boxplot
        
        #DE
        de_data<- attn_whole_set[attn_whole_set$trial_name_2=="de_t_de", ]       
        p_de <-ggplot(data=de_data, aes(x = set_num, y = all_tw_set))      
        p_de <- p_de + geom_boxplot(aes(fill=factor(set_num)))
        #p_de<- p_de + geom_point(position = position_jitter(width = 0.1))
        p_de <- p_de + guides(fill=FALSE)
        p_de <- p_de + scale_x_continuous(breaks = c(1,2,3,4,5,6,7)) +
              scale_fill_manual(breaks = c("1", "2", "3", "4", "5", "6", "7"),
                                values=c(De, De, De, De, De, Ts, De))
       # p_de <- p_de + ylab(element_blank())
       #p_de <- p_de + xlab(element_blank())
        p_de<- p_de + labs(title = "D. ebraccatus / 0-chuck P. pustulosus") +
                  theme(plot.title = element_text(face="italic", size=22)) #this makes the plot title italic, affects the size
         p_de <- p_de + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"), axis.text.y=element_blank(), axis.title.x=element_blank(),
          axis.title.y=element_blank()) #this gets rid of grey background and lines, adds 2 axes
        p_de <- p_de + coord_cartesian(ylim = c(0, 25))
        #p_de <- p_de + stat_smooth(colour="black", size=0.5, method="lm", se=FALSE ,  data= subset(de_data, de_data$set_num!="6" & de_data$set_num!="7")) 
       
        p_de
 ggsave("de_trial.jpg", height =5, width = 7)             
```