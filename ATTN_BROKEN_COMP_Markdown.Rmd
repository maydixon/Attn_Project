---
title: "predictions_attention"
author: "may"
date: "June 29, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Libraries:
```{r}
library(car)
library(dplyr)
library(lme4)
library("aods3")     ## overdispersion diagnostics
library("bbmle")     ## AICtab
library("pbkrtest")  ## parametric bootstrap


```

##GOTTA FIX PRIMARY STIM CODING:


```{r}
attn_habit_3$primary_stim<- attn_habit_3$trial_name_2
attn_habit_3$primary_stim<-recode(attn_habit_3$primary_stim, t_ra_t = "ts")
attn_habit_3$primary_stim<-recode(attn_habit_3$primary_stim, de_t_de = "de", tc_ts_tc = "t2c")
attn_habit_3$primary_stim<-recode(attn_habit_3$primary_stim, tc_ts_tc = "t2c")
attn_habit_3$primary_stim<-recode(attn_habit_3$primary_stim, t_de_t = "ts")

attn_habit_3$primary_stim<-recode(attn_habit_3$primary_stim, tia_tib_tia = "t1c")
attn_habit_3$primary_stim<-recode(attn_habit_3$primary_stim, rt_rra_rt = "rts")
attn_habit_3$primary_stim<-recode(attn_habit_3$primary_stim, ts_tc_ts = "ts")
attn_habit_3$primary_stim<-recode(attn_habit_3$primary_stim, rra_rt_rra = "rra")
attn_habit_3$primary_stim<-recode(attn_habit_3$primary_stim, ra_t_ra = "ra")

```

Scale numeric predictors:
```{r}

#######Ok, trying to fix hessian convergence issues. 
#try to scale numeric predictors (don't think this will work for me but whatever)
#so I am centering the call numbers around 0 and making the variance normal. 
attn_habit_3$scaled_call_num<- scale(attn_habit_3$call_num, center=TRUE, scale=TRUE)  #scaling call number
plot(attn_habit_3$scaled_call_num, attn_habit_3$call_num)

#also going to center call_num_2, which won't have the weird gaps that are weird
attn_habit_3$scaled_call_num_2<- scale(attn_habit_3$call_num_2, center=TRUE, scale=TRUE)  #scaling call number
plot(attn_habit_3$scaled_call_num_2, attn_habit_3$call_num_2) #this one won't have weird arbitrary gaps
```
## Predictions

1. comparing models with and without intercepts, with call as grouping and with set as grouping. 
All of the following in this chunk are not scaled:
```{r}

mm1<- glmer(all_tw_call ~ call_num + call_num:trial_name_2 + (trial_number| Bat_ID), data= attn_habit_3, family="poisson") #run model
smm1<- summary(mm1)  #make summary
smm1
mm1a<- glmer(all_tw_call ~ call_num_2 + call_num_2:trial_name_2 + (trial_number| Bat_ID), data= attn_habit_3, family="poisson") #trying model with a different coding of call number. 
smm1a<- summary(mm1a)  #make summary
smm1a

mm1b <- glmer(all_tw_call ~ call_num  + trial_name_2 + call_num:trial_name_2 + (trial_number| Bat_ID), data= attn_habit_3, family="poisson") #with trial_name_2 as main effect
smm1b<- summary(mm1b)  #make summary
smm1b

mm1c <- glmer(all_tw_call ~ call_num  + trial_name_2 + call_num:trial_name_2 - 1 + (trial_number| Bat_ID), data= attn_habit_3, family="poisson") #run model with trial name as main effect and no intercept
smm1c<- summary(mm1c)  #make summary
smm1c

mm1d <- glmer(all_tw_call ~ call_num  + primary_stim + call_num:primary_stim + (trial_number| Bat_ID), data= attn_habit_3, family="poisson") #run model
smm1d<- summary(mm1d)  #make summary
smm1d # model with trial as main effect, no intercept, primary stim as group 

mm1e <- glmer(all_tw_call ~ call_num  + primary_stim + call_num:primary_stim + (1| Bat_ID), data= attn_habit_3, family="poisson") #run model
smm1e<- summary(mm1d)  #make summary
smm1e # same as mm1d but w/o trial number to see effect on hessian convergence


mm1_5<- glmer(all_tw_call ~ call_num + call_num:trial_name_2 -1 + (trial_number| Bat_ID), data= attn_habit_3, family="poisson") #run model
smm1_5<- summary(mm1_5)  #make summary
smm1_5  #

mm1epsilon <- glmer(all_tw_call ~ call_num  + primary_stim + call_num:primary_stim + (1| Bat_ID), data= attn_habit_3, family="poisson")
smm1epsilon<- summary(mm1epsilon)  #make summary
smm1epsilon
####
####Ok, trying to fix hessian convergence issues. in next chunk

```


Scaled models:


```{r}
#run model with scaled numeric variable, using 1.1...1.5 call_num scale
mm1f <- glmer(all_tw_call ~ scaled_call_num  + primary_stim + scaled_call_num:primary_stim + (1| Bat_ID), data= attn_habit_3, family="poisson")
smm1f<- summary(mm1f)  #make summary
smm1f


#run model with regular integer scale
mm1g <- glmer(all_tw_call ~ scaled_call_num_2  + primary_stim + scaled_call_num_2:primary_stim + (1| Bat_ID), data= attn_habit_3, family="poisson") #run model with scaled numeric variable, using scaled call nums from 1-~35 scale
smm1g<- summary(mm1g
                )  #make summary
smm1g

#run model with the  regular integer call num scale, this time without intercepts
mm1h <- glmer(all_tw_call ~ scaled_call_num  + primary_stim + scaled_call_num:primary_stim -1 + (1| Bat_ID), data= attn_habit_3, family="poisson") #run model with scaled numeric variable, using scaled call nums from 1.1...1.5 scale
smm1h<- summary(mm1h)  #make summary
smm1h

## So basically, scaling the call number to normal does help just a little bit, and is generally a good thing to do. 
#BUt still not converging, what if I add trial_number back in (full model)
#run model with the  decimal call num scale, this time without intercepts:

mm1i <- glmer(all_tw_call ~ scaled_call_num  + primary_stim + scaled_call_num:primary_stim -1 + (trial_number| Bat_ID), data= attn_habit_3, family="poisson") #run model with scaled numeric variable, using scaled call nums from 1.1...1.5 scale
smm1i<- summary(mm1i)
smm1i



#1 centered and scaled
#and with integer scale?    VVVVVVVVVVVVV
#default optimizer, mix of bobyqa and nelder-mead
mm1j <- glmer(all_tw_call ~ scaled_call_num_2*primary_stim -1 + (trial_number| Bat_ID), data= attn_habit_3, family="poisson") #run model with scaled numeric variable, using scaled call nums from integer scale
smm1j<- summary(mm1j)
smm1j  
## not converging

##Troubleshooting, following: 
## critera outlined by the amazing Ben Bolker:
## "Assessing Convergence for Fitted models, convergence {lme4}

##https://stackoverflow.com/questions/33670628/solution-to-the-warning-message-using-glmer
  "Since the likelihood differs by <0.1 between the model fits, and the largest relative differences in the parameters are of the order of about 10^(-4), I would say that you have successfully demonstrated that the warning is a false positive and you can proceed with your initial model."

## https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html ##I use "gold standard" of comparing diff optimizers
  
 #1 center and scale (check) 
#2, check for singularity:
diag.vals <- getME(mm1j,"theta")[getME(mm1j,"lower") == 0]
any(diag.vals < 1e-6) # FALSE , no singularity

##3. recompute gradient and Hessian with Richardson extrapolation
devfun <- update(mm1j, devFunOnly=TRUE)
if (isLMM(mm1j)) {
    pars <- getME(mm1j,"theta")
} else {
    ## GLMM: requires both random and fixed parameters
    pars <- getME(mm1j, c("theta","fixef"))
}
if (require("numDeriv")) {
    cat("hess:\n"); print(hess <- hessian(devfun, unlist(pars)))
    cat("grad:\n"); print(grad <- grad(devfun, unlist(pars)))
    cat("scaled gradient:\n")
    print(scgrad <- solve(chol(hess), grad))
}
## compare with internal calculations:
mm1j@optinfo$derivs 

#So it looks to me that all the scaled gradients in the shorthand calculations are much larger than the lme4 default internal calculations, which would be one reason why model doesn't converge.

## 4. restart the fit from the original value (or
## a slightly perturbed value):
mm1j.restart <- update(mm1j, start=pars) #this also seems to fix the problem. Gets stuck on an optimum?

## 5. try all available optimizers (from Bolker)/ "gold standard"

  source(system.file("utils", "allFit.R", package="lme4"))
  fm1.all <- allFit(mm1j)
  ss <- summary(fm1.all)
  ss$ fixef               ## extract fixed effects #differ at most on the order od 10^-3
  ss$ llik                ## log-likelihoods #check, differ by less than .1
  ss$ sdcor               ## SDs and correlations
  ss$ theta               ## Cholesky factors
  ss$ which.OK            ## which fits worked


  ## This suggests that the model is appropriate, and the convergence warnings are a false positive, bc the different optimizers produce very similar results-- fixed effects that differ on the scale of 10^-3 or less, and identical log-likelihoods (<.1)
 
  ## Here on out I am going to use the bobyqa, though any can be used, bc they all show the name thing. (bootstrap will be vetter if I pick one that converges more, bc more will not fail. ) 
  
#bobyqa omptimizer
mm1bob <- glmer(all_tw_call ~ scaled_call_num_2*primary_stim -1 + (trial_number| Bat_ID), data= attn_habit_3, family="poisson", control=glmerControl(optimizer = "bobyqa"))
smm1bob<- summary(mm1bob)
smm1bob



#other possible optimizers: "Nelder_Mead", the default, etc, also could up the interations:
#upping the iterations (commented out)
# ss <- getME(mm1j,c("theta","fixef"))
# mm1jx <- update(mm1j,start=ss,control=glmerControl(optCtrl=list(maxfun=1e4)))
# smm1jx<-summary(mm1jx)
# smm1jx #fine with 10,000 iterations, doesn't need the max, 20,000 iterations

```
  
  Seems pretty good. Just going to check whether a negative binomial would fit the data better:
  
```{r}
  ####
## test to see if nb.glmer fits betta
mm1bob.nb <- glmer.nb(all_tw_call ~ scaled_call_num_2*primary_stim -1 + (trial_number| Bat_ID), data= attn_habit_3)
smm1bob.nb<- summary(mm1bob.nb)
smm1bob.nb
##
anova(mm1bob.nb, mm1bob)
# It does not. 
######
ANOVA:
Really interested in: does the habituation to calls vary by treatment? e.g. is the call_num/treatment interaction significant

We need type 2 or type 3 anova bc type w doesn't make as much sense here (don't want to move through variables and varience sequentially)
```





```{r}
#anova using car package for type 3 anova
Anova(mm1bob, type=3) # This just demonstrates that is appropriate to further look for an interaction between  call number and primary stimulus. 

```
This justifies that different treatments change at different rates, can look for posthocs. 
So now on to post hoc demonstrating that the different fixed effects are different from one another 
```{r}

## Some bootstapped failures, noted below
bootMer()

boot_habit<-bootMer(x=update(mm1j,start=ss,control=glmerControl(optCtrl=list(maxfun=1e4))),FUN=fixef,nsim=1)
mm1j.ci<- boot.ci(boot.out = boot_habit, type = "perc")

## just making a lot of confidence intervals


c0 <- confint(mm1bob,method="Wald")
```
ran with no errors
```{r}
c1 <- confint(mm1bob)
c1test <- confint(mm1bob) #rerun later
 #in the meantime, so i don't have to rerun it:
c1 <- as.data.frame(read_csv("~/GitHub/Attn_Project/Attn_project_github/c1_boot_output_mac.csv"))
namesc1<-(c1[,1])
c1<-c1[,-1]
rownames(c1)<-namesc1
c1
```
ran with around 78 errors:
1: failure to converge in 10000 evaluations
2:unexpected decrease in profile: using minstep
3:bad spline fit for primary_stimrra: falling back to linear interpolation (or whatever treatment)
  
```{r}
#c2 <- confint(mm1jx,method="boot")  #can't run without way too many failed convergences
##running these two
c2test<- confint(mm1bob, method="boot")
c1test <- confint(mm1bob) #rerun later

```
c2test: 1 run failed, 6 warnings
c2 Warnings:
failure to converge in 10000 evaluations : 43 
expected decrease in profile: using minstep:27
non-monotonic profile for primary_stimde: 6
bad spline fit for primary_stimt1c: falling back to linear interpolation: 6

total: 82
```{r}
library(effects)
library(lsmeans)

eff<-effect("scaled_call_num_2*primary_stim",mm1bob)
eff
eff$lower

c3 <- with(Effect(c("scaled_call_num_2", "primary_stim"),mm1bob),cbind(lower,upper)) #this is pulling up more # than its supposed to
c3
#c4 <- with(summary(lsmeans(mm1bob,spec="scaled_call_num_2*primary_stim")),cbind(lower.CL,upper.CL)) #can't get this to work
tmpf <- function(method,val) {
    data.frame(method=method,
               v=names(fixef(mm1bob)),
               setNames(as.data.frame(tail(val,14)),
                        c("lwr","upr")))
}
library(ggplot2); theme_set(theme_bw())
allCI <- rbind(tmpf("lme4_wald",c0),
      tmpf("lme4_prof",c1),
      tmpf("lme4_boot",c2test)
      #tmpf("effects",c3)
      )

confint_comp<- ggplot(allCI,aes(v,ymin=lwr,ymax=upr,colour=method))+
    geom_linerange(position=position_dodge(width=0.8)) +
                     theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))

confint_comp


#######
#messing around with above
#cutting out extra non-interaction values
tmpf <- function(method,val) {
    data.frame(method=method,
               v=names(fixef(mm1bob))[c(1,9:14)],
               setNames(as.data.frame(val[c(4,12:17),]), #pulls relevent rows
                        c("lwr","upr"))
               setNames(as.data.frame(val[c(4,12:17),]), #pulls relevent rows
                        c("lwradj","upradj")))
}

as.data.frame(c2test[c(4,12:17),])[1+c(2:7),]
apply()
as.list(as.data.frame(c2test[c(4,12:17),])[1,])
(as.data.frame(c2test[c(4,12:17),])+as.list(as.data.frame(c2test[c(4,12:17),])[1,]))


#####THis is horribly ugly. trying to make 2 rows that add the value of de to get the true slope. trying to add the row with the scaled call number to the value of the scaled call number added to all the other rows. 
cbind( as.data.frame( rbind( as.data.frame(c2test)[1,]  , ((as.data.frame(c2test[c(12:17),]))[,1]+ (as.data.frame(c2test))[1,1] )) , as.data.frame(as.data.frame(c2test[c(12:17),]))[,2]+ (as.data.frame(c2test))[1,2])
```
```{r}
library(ggplot2); theme_set(theme_bw())
allCI <- rbind(tmpf("lme4_wald",c0),
      tmpf("lme4_prof",c1),
      tmpf("lme4_boot",c2test)
)

confint_comp<- ggplot(allCI,aes(v,ymin=lwr,ymax=upr,colour=method))+
    geom_linerange(position=position_dodge(width=0.8)) +
                     theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))

confint_comp


#######
               


ggsave("pastes_confint_mac.png",width=10)

boot_habit2<-bootMer(x=mm1jx,FUN=fixef,nsim=1000)

c0df<- as.data.frame(c0)
c1df<- as.data.frame(c1)
c2df<- as.data.frame(c2)
c3df<- as.data.frame(c3)
c2dftest<- as.data.frame(c2test)

write.csv(c0df, file="c0_boot_ouput_mac.csv")
write.csv(c1df, file="c1_boot_output_mac.csv")
write.csv(c2df, file="c2_boot_output_mac.csv")
write.csv(c3df, file="c3_boot_output_mac.csv")
write.csv(c2dftest, file="c2test_boot_output_mac.csv")



```
trying to do post-hoc comparisons to compare if the interactions are different from one another 

```{r}
#trying to do post-hoc comparisons to compare if the interactions are different


library(multcomp)
library(multcompView)

#     list estimates, se, df, 95CIS
lst=lsmeans::lstrends(mm1bob, ~primary_stim, var="scaled_call_num_2")


#     Show which are different from one another in letter form:
sig_vals_mm1bob<-cld(lst)

#     Show pairwise comparisons:
pairs_mm1bob<- pairs(lst)

#With back transformed values ( I guess these are the 'truest' ones)

lst2=lsmeans::lstrends(mm1bob, ~primary_stim, var="scaled_call_num_2", transform = "response")
sum_lst2<-summary(lst2)
sum_lst2     #makes a nice dataframe

sv2<-cld(lst2)
pairs_lst2<- pairs(lst2)
s_pairs_lst2<-summary(pairs_lst2)

##    write.csv(sv2, file="habituation_glmm_significant_diffs.csv")
##    write.csv(sum_lst2, file="habituation_glmm_model_estimates.csv")
##    write.csv(pairs_lst2, file="habituation_glmm_pairwise_comps.csv")
sv2$method<-"lsmeans_conf"

##plotz##
plotlst2<-plot(lst2, ~primary_stim, var="scaled_call_num_2", comparisons = TRUE, alpha = .05)
plotlst<-plot(lst, ~primary_stim, var="scaled_call_num_2", comparisons = TRUE, alpha = .05)
par(2)
plotlst
plotlst2

library(ggpubr); theme_set(theme_classic())

confint_comp_ls2<- ggplot(sv2, aes(primary_stim,ymin=asymp.LCL,ymax=asymp.UCL,colour=primary_stim))+
    geom_linerange() +
 theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black")) + #gets rid of background
 xlab("Call Type") +
ylab("Slope of change in response to calls)")

                     
ggsave
confint_comp_ls2
ggsave("habituation_slope_plots.png",width=10)
```

```{r}

#Alternatively, TO CORRECT P-vals using straight output: instead of using 0.05 / number of treatments #adjusting significance
#0.05 but as you go, change the significance level Bonefferoni Holm 
#


# following advice from https://stats.stackexchange.com/questions/5250/multiple-comparisons-on-a-mixed-effects-model
```
Show in New WindowClear OutputExpand/Collapse Output
Loading required package: Matrix
Loading required package: boot

Attaching package: ‘boot’

The following object is masked from ‘package:car’:

    logit

Loading required package: stats4

Attaching package: ‘bbmle’

The following object is masked from ‘package:dplyr’:

    slice

Show in New WindowClear OutputExpand/Collapse Output
R Console


Show in New WindowClear OutputExpand/Collapse Output
Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: poisson  ( log )
Formula: all_tw_call ~ scaled_call_num_2 * primary_stim - 1 + (trial_number |  
    Bat_ID)
   Data: attn_habit_3
Control: glmerControl(optimizer = "bobyqa")

     AIC      BIC   logLik deviance df.resid 
  5826.9   5928.8  -2896.4   5792.9     2953 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.3878 -0.6094 -0.2584  0.4085 20.4459 

Random effects:
 Groups Name         Variance Std.Dev. Corr
 Bat_ID (Intercept)  0.07862  0.2804       
        trial_number 0.02480  0.1575   0.21
Number of obs: 2970, groups:  Bat_ID, 11

Fixed effects:
                                  Estimate Std. Error z value Pr(>|z|)    
scaled_call_num_2                  0.05044    0.06987   0.722  0.47032    
primary_stimde                     0.48248    0.15872   3.040  0.00237 ** 
primary_stimra                    -0.80968    0.18859  -4.293 1.76e-05 ***
primary_stimrra                   -0.11417    0.15389  -0.742  0.45817    
primary_stimrts                    0.04486    0.15264   0.294  0.76886    
primary_stimts                     0.07365    0.15124   0.487  0.62627    
primary_stimt2c                    0.53038    0.15262   3.475  0.00051 ***
primary_stimt1c                    0.11568    0.15537   0.745  0.45655    
scaled_call_num_2:primary_stimra  -1.07187    0.12025  -8.914  < 2e-16 ***
scaled_call_num_2:primary_stimrra -0.86673    0.09873  -8.779  < 2e-16 ***
scaled_call_num_2:primary_stimrts -0.41473    0.10785  -3.845  0.00012 ***
scaled_call_num_2:primary_stimts  -0.27230    0.08434  -3.228  0.00124 ** 
scaled_call_num_2:primary_stimt2c -0.17685    0.09734  -1.817  0.06925 .  
scaled_call_num_2:primary_stimt1c -0.29098    0.09030  -3.222  0.00127 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Correlation matrix not shown by default, as p = 14 > 12.
Use print(x, correlation=TRUE)  or
	 vcov(x)	 if you need it

Show in New WindowClear OutputExpand/Collapse Output
Analysis of Deviance Table (Type III Wald chisquare tests)

Response: all_tw_call
                                  Chisq Df Pr(>Chisq)    
scaled_call_num_2                0.5212  1     0.4703    
primary_stim                   158.7958  7     <2e-16 ***
scaled_call_num_2:primary_stim 144.7704  6     <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Show in New WindowClear OutputExpand/Collapse Output
 
 
2.5 %
<dbl>
97.5 %
<dbl>
.sig01	0.14617987	0.54354624		
.sig02	-0.66082778	0.82825728		
.sig03	0.10389497	0.26316698		
scaled_call_num_2	-0.08577055	0.18329547		
primary_stimde	0.17735637	0.80632931		
primary_stimra	-1.20829705	-0.44852126		
primary_stimrra	-0.44751931	0.14361949		
primary_stimrts	-0.28461186	0.35740028		
primary_stimts	-0.19919564	0.36944067		
primary_stimt2c	0.21446474	0.81934830		
Next12Previous
1-10 of 17 rows
Modify Chunk OptionsRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current Chunk
Show in New WindowClear OutputExpand/Collapse Output
Computing bootstrap confidence intervals ...
Model failed to converge with max|grad| = 0.0394785 (tol = 0.001, component 1)Model failed to converge with max|grad| = 0.0280402 (tol = 0.001, component 1)Model failed to converge with max|grad| = 0.0295315 (tol = 0.001, component 1)unable to evaluate scaled gradientModel failed to converge: degenerate  Hessian with 1 negative eigenvaluesModel failed to converge with max|grad| = 0.0414394 (tol = 0.001, component 1)Model failed to converge with max|grad| = 0.0350131 (tol = 0.001, component 1)some bootstrap runs failed (1/500)
Show in New WindowClear OutputExpand/Collapse Output

 scaled_call_num_2*primary_stim effect
                 primary_stim
scaled_call_num_2       de         ra       rra       rts        ts      t2c       t1c
               -1 1.540391 1.23583321 2.0180327 1.5055293 1.3438064 1.928589 1.4279177
               0  1.620083 0.44500098 0.8921097 1.0458783 1.0764308 1.699576 1.1226331
               1  1.703898 0.16023673 0.3943740 0.7265628 0.8622545 1.497757 0.8826174
               2  1.792048 0.05769832 0.1743405 0.5047369 0.6906927 1.319904 0.6939164
Modify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current ChunkModify Chunk OptionsRun All Chunks AboveRun Current Chunk
Show in New WindowClear OutputExpand/Collapse Output

Show in New WindowClear OutputExpand/Collapse Output
NOTE: Results may be misleading due to involvement in interactions

Console~/GitHub/Attn_Project/Attn_project_github/
			

    > 
    > model1 <- lme(weight ~ Time * Diet, random = ~1|Rat/Time,  data=ex)    
    > model2 <- lme(weight ~ TimeDiet, random = ~1|Rat/Time, data=ex)    



```
messing around with making bootstap make sense

```{r}
#second case more complex design with two crossed RE and a poisson response

 
#for GLMMs we have to back-transform the prediction after adding/removing the SE
newdat<-data.frame(x=seq(1,14,length=14))
mm<-model.matrix(~x,newdat)
y<-mm%*%fixef(mm1jx)
pvar1 <- diag(mm %*% tcrossprod(vcov(m),mm))
tvar1 <- pvar1+VarCorr(m)$f1[1]+VarCorr(m)$f2[1]  ## must be adapted for more complex models
newdat <- data.frame(
  x=newdat$x,
  y=exp(y),
   plo = exp(y-1.96*sqrt(pvar1))
  , phi = exp(y+1.96*sqrt(pvar1))
  , tlo = exp(y-1.96*sqrt(tvar1))
  , thi = exp(y+1.96*sqrt(tvar1))
)
 
#second version with bootMer
predFun<-function(.) exp(mm%*%fixef(.)) 
bb<-bootMer(m,FUN=predFun,nsim=200)
bb_se<-apply(boot_habit$t,2,function(x) x[order(x)])
newdat$blo<-bb_se[1,]
newdat$bhi<-bb_se[2,]
newdatz<-data.frame(seq(1,14, length=14))
newdat$names <- names(fixef(mm1jx))

newdatz
```

effects(scaled_call_num_2*primary_stim, mm1j) #this isn't called right
## This is the best model
## It is a full model, has all the main effects of call number and of the interaction effects
## It has both trial number and subject as random effects
## It converges
## It's "time" value is normalized and scaled (centered, etc), which is good bc the names for time are arbitrary anyway
## We used the call number scale that makes the most sense, isn't interpreted as different weights of differences between each chunk. 



## This is just for me to look at what the values of the slope are for the interactions:
smm1jnames<- rownames(smm1j$coefficients)
Estimate1<- smm1j$coefficients + smm1j$coefficients[1,1] #adding the main effect of call number, using De as reference. 
cbind(smm1jnames, as.numeric(Estimate1))


##Hookay. So now I have a model with some values that are things. I can try to estimate the confidence intervals now, I believe. 
#These are estimated confindence intervals:
confint_full_model<- confint(mm1j, type="bca")
CFM<- confint_full_model

#Now to make a dataframe with the fixed effect values of the interactions, and their confidence intervals

#now I am going to try to make a dataframe including only the interaction effects, and really, only the interaction effects when added to the reference value, so the true values of each of the interactions
# fixef_mm1j <- cbind(fixef(mm1j))
# fixef_mm1j_ie<-rbind(fixef_mm1j[1,],cbind(fixef_mm1j[9:14,] + fixef_mm1j[1,])) #deleting the no interaction effect fixed effects
# colnames(fixef_mm1j_ie)<- "fixeffects"
# fixef_mm1j_ie     
#       
# #Doing the same to the confidence intervals
# conf_frame<-rbind(CFM[4,], (CFM[12:17,] + CFM[4,])) #making all interactions true values by adding the main effect of call number which is referenced to de to the other values
# rownames(conf_frame)[1]<- "scaled_call_num_2:primary_stimde" #adding rowname to the first column
# TMT<-c("De", "Ra", "Rra", "Rt1c", "T1c", "T2c", "Ts") 
# row.names(conf_frame)<-NULL
# interaction_cis<-as.data.frame(cbind(TMT, fixef_mm1j_ie, conf_frame)) #data frame with the 95%cis for the interactions bectween call number and treatments
# interaction_cis$`2.5 %`<-as.numeric(as.character(interaction_cis$`2.5 %`))
# interaction_cis$`97.5 %`<-as.numeric(as.character(interaction_cis$`97.5 %`))
# interaction_cis$fixeffects<-as.numeric(as.character(interaction_cis$fixeffects))
# colnames(interaction_cis)<- c("TMT", "fixeffects", "lower_ci", "upper_ci")
# interaction_cis$midval<- apply(interaction_cis[,3:4],1, mean)
# row.names(interaction_cis) <- NULL 
# interaction_cis


### Alternatively, just use Tidy?
Mm1j_tidy<- tidy(mm1j, conf.int=TRUE)
#makes lovely dataframe with all my lovely values

#cleaning a bit:
MM1j_ceoffs<- Mm1j_tidy[-2:-8,]
MM1j_coeffss<- MM1j_ceoffs[-8:-10,]
MM1j_coeff3<- cbind(TMT, MM1j_coeffss)

#now to adjust for reference value. Make new columns adding reference to the interactions

MM1j_coeff3$slopes<- MM1j_coeff3[,3] + 0.05044288 #adding value of De/reference to the slopes
MM1j_coeff3$slopes[1]<-0.05044288 #fixing the effect for de
MM1j_coeff3$conf.low.adj<-MM1j_coeff3$conf.low + 0.05044288 #adding value of De/reference to the low.ci
MM1j_coeff3$conf.low.adj[1]<- -0.08649563 #fixing high ci for de

MM1j_coeff3$conf.high.adj<-MM1j_coeff3$conf.high + 0.05044288 #adding value of De/reference to the low.ci
MM1j_coeff3$conf.high.adj[1]<- 0.1873814 #fixing high ci for de

```
Now to graph these

```{r}
#graphing the slopes and their CI's

plot_ci_slopes<- ggplot(data= MM1j_coeff3, aes(x=TMT, y= slopes)) +
geom_point() +
geom_errorbar(ymin = MM1j_coeff3$conf.low.adj  , ymax = MM1j_coeff3$conf.high.adj )
plot_ci_slopes

plot_ci_slopes<- ggplot(data=interaction_cis,  aes(x=(lower_ci+upper_ci/2))) +
geom_errorbar(ymin = interaction_cis$lower_ci  , ymax = interaction_cis$upper_ci )

plot_ci_slopes
```
Show in New WindowClear OutputExpand/Collapse Output
estimate
<dbl>
std.error
<dbl>
statistic
<dbl>
p.value
<dbl>
conf.low
<dbl>
conf.high
<dbl>
group
<chr>
0.05044288	0.06986787	0.7219753	4.703096e-01	-0.08649563	0.1873814	fixed
-1.07186301	0.12024991	-8.9136282	4.938922e-19	-1.30754850	-0.8361775	fixed
-0.86674770	0.09873082	-8.7788968	1.650859e-18	-1.06025655	-0.6732388	fixed
-0.41473021	0.10785571	-3.8452320	1.204385e-04	-0.62612350	-0.2033369	fixed
-0.25967210	0.08068247	-3.2184451	1.288877e-03	-0.41780684	-0.1015374	fixed
-0.17686792	0.09734415	-1.8169343	6.922717e-02	-0.36765894	0.0139231	fixed
-0.36817611	0.10992532	-3.3493293	8.100745e-04	-0.58362578	-0.1527264	fixed
7 rows | 3-9 of 8 columns

##################



names<-c("smm1$AICtab", "smm1_5$AICtab", "smm2$AICtab", "smm2_5$AICtab", "smm3$AICtab", "smm3_5$AICtab", "smm4$AICtab", "smm4_5$AICtab, "smm5")
AIC_comp<- rbind( smm1$AICtab, smm1_5$AICtab, smm2$AICtab, smm2_5$AICtab, smm3$AICtab, smm3_5$AICtab, smm4$AICtab, smm4_5$AICtab, smm5)
AIC_comp<- cbind(names, AIC_comp)
AIC_comp

coeff_comp<- cbind(smm1$coefficients, smm1_5$coefficients, smm2$coefficients, smm2_5$coefficients, smm3$coefficients, smm3_5$coefficients, smm4$coefficients, smm4_5$coefficients)
coeff_comp




```



```{r}
library(lme4)
library(boot)
#2.5 #this adds call number as a main effect, but doesn't include the main effect of trial type
m2_5<- glmer(all_tw_call ~ call_num + call_num:trial_name_2 + trial_number + (1| Bat_ID), data= attn_habit_3, family="poisson")
sm25<- summary(m2_5)
sm25

boot_habit<-bootMer(x=sm25,FUN=fixef,nsim=200)
boot.ci(b_par,type="perc",index=1)




```